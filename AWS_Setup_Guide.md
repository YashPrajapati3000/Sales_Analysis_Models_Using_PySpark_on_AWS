# ⚙️ AWS Setup Guide for Sales Analysis Project

This guide outlines the AWS configuration required to run the **Sales Analysis Models Using PySpark** project on cloud-scale infrastructure using EMR and S3.

---

## 🗂️ S3 Bucket Structure

Create an S3 bucket (e.g., `sales-analysis-beverage-bucket`) and organize it as follows:

**s3://sales-analysis-beverage-bucket/**
- **raw/**: Raw CSV input data
- **processed/**: Parquet files output after PySpark processing
- **athena-results/**: Query results generated by Athena

**Upload Instructions:**
- Upload your original dataset (`beverage_sales_data.csv`) to the `raw/` folder.

---

## 🛠️ EMR Cluster Setup

1. **Launch Cluster:**
   - AWS Console → EMR → Create Cluster → Advanced Options

2. **Software Configuration:**
   - EMR Release: `6.10.0` or later
   - Applications: ✅ Hadoop, ✅ Spark

3. **Instance Configuration:**
   - Master Node: `m4.xlarge` (1)
   - Core Nodes: `m4.xlarge` (2 or more based on scale)

4. **Cluster Settings:**
   - Cluster Name: `SalesAnalysisEMR`
   - Logging: (Optional)
   - EC2 Key Pair: Select or create one for SSH access
   - Keep cluster alive when no steps are running: ✅ Enabled

5. **IAM Role Permissions:**
   - Ensure EMR EC2 instance role has full access to your specified S3 bucket.

---

## ✅ Notes

- **Athena Setup:** All Athena-related configurations and queries are automated and handled programmatically within the Jupyter notebook.
- **Secrets:** Ensure that AWS credentials are **not hardcoded** in notebooks. Use IAM roles or secure environment variables.

---

> For full project details and visualizations, see [README.md](README.md).
